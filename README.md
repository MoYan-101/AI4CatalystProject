# Catalyst ML Pipeline
This repo trains ML models to predict catalyst performance using numeric features
plus material and text embeddings. It includes training, inference, and
visualization in one pipeline.

## Main Workflow
1. Data loading and preprocessing (`data_preprocessing/data_loader_modified.py`)
   - Cleans CSV, normalizes missing tokens, builds numeric + material/text embeddings.
   - Generates feature matrix `X`, targets `Y`, and metadata.
2. Training (`train.py`)
   - Optional Optuna tuning.
   - 5-fold CV metrics and model training for RF/DT/CatBoost/XGB/ANN/SVM.
   - Saves models, scalers, and SHAP data.
3. Inference (`inference.py`)
   - Loads trained models and metadata.
   - Generates 2D heatmaps, optional 3D heatmaps, and confusion-like matrices.
4. Visualization (`visualization.py`)
   - Plots CV metrics, residuals, SHAP, heatmaps, and other figures.

## Quick Start
1) Edit `configs/config.yaml` (data path, model list, Optuna settings).
2) Run:
```bash
bash run.sh
```

## Run Options
`run.sh` will prompt for `overfit_penalty_alpha` values.
- Enter one or multiple values (comma-separated), e.g. `0.0,0.03`.
- Press Enter to use the value in `configs/config.yaml`.
- Each alpha gets its own `RUN_ID` suffix to avoid overwriting results.

You can skip the prompt:
```bash
OVERFIT_ALPHA_LIST="0.0,0.03" bash run.sh
```

## Config Highlights
- `data.path`: CSV file path.
- `data_loader.impute_method`: `kde` or `simple` missing value handling.
- `data_loader.preserve_null`: keep "Null" as a valid category.
- `data_loader.element_embedding`: `advanced` (uses AdvancedMaterialFeaturizer) or `simplified`/`basic`.
- `data_loader.promoter_ratio_cols`: ratio columns aligned with `element_cols` (Promoter 1/2).
- `data_loader.promoter_onehot`: add promoter identity one-hot features.
- `preprocessing.standardize_all_features`: scale all feature columns (numeric + embeddings + one-hot).
- `optuna.overfit_penalty_alpha`: overfit penalty weight.
- `inference.heatmap_axes`: 2D or 3D axes for heatmaps.
- `inference.enable_3d_heatmap`: toggle 3D heatmap generation.
- `inference.skip_3d_models`: skip 3D heatmap for listed models (e.g. SVM).

## Data Processing Details (current pipeline)
The default pipeline is implemented in `data_preprocessing/data_loader_modified.py`:

1) **Input columns**
   - Uses all columns except Y and metadata (`DOI`, `Name`, `Year` by default).
   - Y columns default to:
     - `CO selectivity (%)`
     - `Methanol selectivity (%)`
     - `STY_CH3OH (g/kg·h) (LN scale)`
     - `CO2 conversion efficiency (%)`

2) **Missing handling**
   - Common missing tokens (e.g., `NaN`, `None`, empty) → `np.nan`.
   - `"Null"` is **not** treated as missing; it is a valid category.
   - Optional KDE or simple imputation for missing values (`data_loader.impute_method`).

3) **Promoter features (per Promoter column)**
   - **Base material vector** via `AdvancedMaterialFeaturizer` (requires `pymatgen` + `matminer`).
   - **Ratio-weighted vector**: `magpie * promoter_ratio`.
   - **Explicit Null flag**: `is_null` (1 if value is `"Null"`).
   - **Optional one-hot identity** (`promoter_onehot: true`).
   - Final promoter block: `[magpie | magpie*ratio | is_null | onehot]`.

4) **Text features**
   - Text columns are encoded as **one-hot** (preserves discrete synthesis types).

5) **Scaling**
   - With `preprocessing.standardize_all_features: true`, all feature columns
     (numeric + embeddings + one-hot) are standardized together.

## Outputs
- `models/<csv_name>/<run_id>/...` trained models, scalers, metadata.
- `postprocessing/<csv_name>/<run_id>/...` training artifacts and inference arrays.
- `evaluation/figures/<csv_name>/<run_id>/...` plots and summaries.

## Notes
- 3D heatmaps can be slow. Use `skip_3d_models` and `max_combinations` to control cost.
- SVM SHAP uses KernelExplainer and can be slow on large datasets.
